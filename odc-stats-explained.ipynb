{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c2dc7f8-8b93-4468-b648-86461e3b85d3",
   "metadata": {},
   "source": [
    "# Creating an odc-stats plugin; or how I learned to stop worrying and love odc-stats\n",
    "\n",
    "> Important: for this notebook to work in the Sandbox, both odc-stats and odc-algo require upgrading\n",
    "\n",
    "Useful links:\n",
    "* [odc-stats](https://github.com/opendatacube/odc-stats)\n",
    "* [odc-stats plugins](https://github.com/opendatacube/odc-stats/tree/develop/odc/stats/plugins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d389c4-f808-4348-82dc-f15f5acce985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/opendatacube/odc-algo.git\n",
    "# !pip install -U odc-stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb55d4-ef2b-4cbd-8b86-135fe8eb8a74",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "**ODC-Statistician** ([Open Data Cube Statistician](https://github.com/opendatacube/odc-stats)) is a framework of tools for generating statistical summaries (usually across time) of large collections of Earth Observation imagery managed in an Open Datacube Instance. \n",
    "\n",
    "`odc-stats` is a powerful and flexible tool for running batch processing of tiled Earth Observation summary products across many EC2 instances on a cloud compute environment. However, for an Earth Observation scientist who is more familiar with developing projects on a single machine (think the Sandbox), it can be confusing to transition code to odc-stats.  This is partly because of a lack of documentation of odc-stats functions, which requires reading source code to understand how it works, and partly because the structure and nomenclature of odc-stats differs from the usual parlance of EO scientists working within the Sandbox ecosystem.  \n",
    "\n",
    "**Importantly**, this notebook is not intended for developers looking for instructions on how to coordinate a large scale batch run of odc-stats on kubernetes. Instead, its intention is to <ins>provide guidance to EO scientists on how to translate their code from the Sandbox to odc-stats</ins>.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4483b26-f41f-49ea-a230-c6d9ca6ed0cf",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "**The aims of this notebook are two-fold**:\n",
    "1. Demystify the use of odc-stats by demonstrating development of a minimal odc-stats \"plugin\".\n",
    "2. Provide example code for running odc-stats functions within a local machine, thus demonstrating how to develop and test the development of an odc-stats \"plugin\".\n",
    "\n",
    "**Two key things are required to use odc-stats.** A `plugin`, which is essentially a python class that contains functions for summarising satellite images within an ODC environment. And a `config` file (a .yaml file) which provides arguments to the plugin.  In this notebook we will systematically build these files.\n",
    "\n",
    "The notebook is broken up into <ins>two main sections</ins>.\n",
    "1. Firstly, we will develop a simple odc-stats plugin, and run it _without using odc-stats_. This can be useful for testing purposes when developing a function, and it will also help us understand the transition from 'sandbox-esque' code to odc-stats code.\n",
    "2. In the second section, we will demonstrate running the plugin using odc-stats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9185bb-316b-479d-b4a9-7150de03259f",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539dacf-fce1-4f51-9ab1-ad29c399cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import warnings\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import geopandas as gpd\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from odc.geo.xr import assign_crs\n",
    "from odc.stats.tasks import TaskReader\n",
    "from odc.stats.model import OutputProduct\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f63dac-4aca-4141-8633-1041e64ecabf",
   "metadata": {},
   "source": [
    "## Section 1: Create a plugin, run it _without_ using odc-stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5a6bf-fb5a-4762-9d4a-e7ceefb8b38d",
   "metadata": {},
   "source": [
    "### Creating an odc-stats plugin\n",
    "\n",
    "To begin, let's create a python `StatsPluginInterface` class function that summarises a Landsat time series of NDVI.\n",
    "\n",
    "The base `StatsPluginInterface` python class is described [here](https://github.com/opendatacube/odc-stats/blob/develop/odc/stats/plugins/_base.py).  To define a custom plugin, we generally need to define a few key functions:\n",
    "\n",
    "1. <ins>`measurements`</ins>: a function that describes the output 'measurements' of the final product (AKA 'bands'). In this example, the final product will have a measurement called 'ndvi_median'\n",
    "   \n",
    "2. <ins>`native_transform`</ins>: this function is passed to an upstream function called [odc.algo.io.load_with_native_transform](https://github.com/opendatacube/odc-algo/blob/bd2fb6828beafed60b5f58f465df8da78cb071e2/odc/algo/io.py#L157). The role of this function is to define pre-processing steps that are applied to individually to every satellite image. This is usually used for things like masking cloud, nodata, and contiguity masking.  The 'load_with_native_transform' function sits within a higher order function called [input_data](https://github.com/opendatacube/odc-stats/blob/7f34c86bdbd481340c41b5be7e0d0873ce3b3e1c/odc/stats/plugins/_base.py#L65) (which itself is within the `StatsPluginInterface` class). For relatively standard odc-stats operations where the main tasks are loading, masking, and summarising a time series of satellite images, defining a \"native_transform\" function for masking is all that's required (and this is passed to \"input_data\"). However, for more flexibility, we can define our own custom \"input_data\" function and odc-stats will run this instead. For example, see [this plugin](https://github.com/opendatacube/odc-stats/blob/develop/odc/stats/plugins/lc_ml_treelite.py) that runs a machine learning prediction. \n",
    "\n",
    "3. <ins>`reduce`</ins>: This function describes how we summarise a time series to a single image. For example, by taking a temporal median. However, this function can be highly flexible. For example, we could load a machine learning model in this step to classify data. We could even load other ancillary datasets if needed.\n",
    "\n",
    "\n",
    "Now, lets create a simple `StatsPlugin` class that, when provided with a series of DEA Landsat images, will mask for clouds and bad data, calculate NDVI, and then 'reduce' the time series using a temporal median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0fa3ad-bce2-43c6-8c0e-1147462bad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#masking functions\n",
    "from typing import Optional, Sequence, Tuple\n",
    "from datacube.utils.masking import mask_invalid_data\n",
    "from odc.algo._masking import (\n",
    "    enum_to_bool,\n",
    "    mask_cleanup,\n",
    "    erase_bad\n",
    ")\n",
    "#odc-stats functions for registering a plugin\n",
    "from odc.stats.plugins._registry import register, StatsPluginInterface\n",
    "\n",
    "class StatsNDVI(StatsPluginInterface):\n",
    "    \"\"\"\n",
    "    Define a class for summarising time \n",
    "    series of NDVI using the median.\n",
    "    \"\"\"\n",
    "    \n",
    "    NAME = \"ndvi_median\"\n",
    "    SHORT_NAME = NAME\n",
    "    VERSION = \"1.0\"\n",
    "    PRODUCT_FAMILY = \"ndvi\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_bands: Sequence[str] = None,\n",
    "        output_bands: Sequence[str] = None,\n",
    "        mask_band: Sequence[str] = None,\n",
    "        contiguity_band: Sequence[str] = None,\n",
    "        group_by: str = \"solar_day\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        self.input_bands = input_bands\n",
    "        self.output_bands = output_bands\n",
    "        self.mask_band = mask_band\n",
    "        self.contiguity_band = contiguity_band\n",
    "        self.group_by = group_by\n",
    "\n",
    "        ## These params get passed to the upstream \n",
    "        #  base StatsPluginInterface class\n",
    "        super().__init__(\n",
    "            input_bands=tuple(input_bands)+(mask_band,)+(contiguity_band,),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def measurements(self) -> Tuple[str, ...]:\n",
    "        \"\"\"\n",
    "        Here we define the output bands, in this example we\n",
    "        will pass the names of the output bands into the config file,\n",
    "        but equally we could define the outputs names within this function.\n",
    "        For example, by adding a suffix to the input bands.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.output_bands\n",
    "\n",
    "    def native_transform(self, xx):\n",
    "        \"\"\"\n",
    "        This function is passed to an upstream function\n",
    "        called \"odc.algo.io.load_with_native_transform\".\n",
    "        The function decribed here is applied on every time\n",
    "        step of data and is usually used for things like\n",
    "        masking clouds, nodata, and contiguity masking.\n",
    "        \"\"\"\n",
    "        #grab the QA band from the Landsat data\n",
    "        mask = xx[self.mask_band]\n",
    "\n",
    "        # create boolean arrays from the mask for cloud\n",
    "        # and cloud shadows, and nodata\n",
    "        bad = enum_to_bool(mask, (\"nodata\",))\n",
    "        non_contiguent = xx.get(self.contiguity_band, 1) == 0\n",
    "        bad = bad | non_contiguent\n",
    "        \n",
    "        cloud_mask = enum_to_bool(mask, (\"cloud\", \"shadow\"))\n",
    "        bad =  cloud_mask | bad\n",
    "\n",
    "        # drop masking bands\n",
    "        xx = xx.drop_vars([self.mask_band] + [self.contiguity_band])\n",
    "        \n",
    "        ## Mask the bad data (clouds etc)\n",
    "        xx = erase_bad(xx, bad)\n",
    "\n",
    "        return xx\n",
    "\n",
    "    def reduce(self, xx: xr.Dataset) -> xr.Dataset:\n",
    "        \"\"\"\n",
    "        Calculate NDVI and summarise time series with a median.\n",
    "        \"\"\"\n",
    "        # convert to float and convert nodata to NaN so NDVI\n",
    "        # isn't calculated on the nodata integer values\n",
    "        xx = mask_invalid_data(xx)\n",
    "        \n",
    "        # Calculate NDVI\n",
    "        ndvi = (xx['nbart_nir'] - xx['nbart_red']) / (xx['nbart_nir'] + xx['nbart_red'])\n",
    "\n",
    "        # calculate temporal median NDVI. \n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!! \n",
    "        # Note that we use 'spec' here and not 'time', this is an odc-stats thing\n",
    "        # where the dimensions are labelled as spec, x, and y.\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        ndvi = ndvi.median('spec').rename(self.output_bands)\n",
    "        \n",
    "        return ndvi.to_dataset()\n",
    "\n",
    "# now lets 'register' the function with odc-stats\n",
    "register(\"NDVI-median\", StatsNDVI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b92667-3c34-47dc-92d7-f35d8a79523b",
   "metadata": {},
   "source": [
    "### Create a config\n",
    "\n",
    "We need to create a config to describe the input parameters for the plugin, for now, we will create a dictionary. However, when it comes time to run the function with odc-stats, this is usually stored in a .yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d8a69-34fb-496f-b03f-787e2be92023",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    input_bands=[\"nbart_red\",  \"nbart_nir\"],\n",
    "    output_bands = 'ndvi_median',\n",
    "    mask_band=\"oa_fmask\",\n",
    "    contiguity_band='oa_nbart_contiguity'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c53942-2d51-4194-8be0-9131dacea928",
   "metadata": {},
   "source": [
    "### Run the plugin code\n",
    "\n",
    "To test our plugin, we will load datasets in a way that's more familiar to those used to working on the Sandbox. These steps mimic the inputs that odc-stats expects, but we load them in a more 'conventional' way than odc-stats (more on this in the next section).\n",
    "\n",
    "Below we define an analysis area, and set up a ODC query. Then we load a list of datasets and the geobox that describes the geographical extent of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2649cf3d-3584-4acb-bf35-5666cfb40ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "dc = datacube.Datacube(app=\"odc-stats example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b41a6d-1138-4e78-8c5e-33cd6f115e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis params\n",
    "lat, lon = -34.134, 140.747\n",
    "buffer = 0.05\n",
    "time_range = ('2024')\n",
    "resolution = (-30, 30)\n",
    "\n",
    "lat_range = (lat-buffer, lat+buffer)\n",
    "lon_range = (lon-buffer, lon+buffer)\n",
    "\n",
    "#set up query object\n",
    "query = {\n",
    "    'x': lon_range,\n",
    "    'y': lat_range,\n",
    "    'time': time_range,\n",
    "    'resolution': resolution,\n",
    "    'output_crs':'epsg:3577',\n",
    "    'measurements':['nbart_red','nbart_nir','oa_fmask']\n",
    "}\n",
    "\n",
    "# load some data, but we'll just extract the geobox parameter\n",
    "# because 'input_data' expects a geobox\n",
    "gbox = dc.load(product=['ga_ls8c_ard_3'], dask_chunks={}, **query).geobox\n",
    "\n",
    "# load a list of datatsets to mimic odc-stats cached \".db\" files\n",
    "dss = dc.find_datasets(product=['ga_ls8c_ard_3'], **query)\n",
    "\n",
    "print('Number of datasets:', len(dss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba8eab-04a8-48d1-bb38-33dea1474fdd",
   "metadata": {},
   "source": [
    "### Run the plugin functions\n",
    "\n",
    "These are evaluated 'lazily' with dask, so will evaluate quickly. Once we run `.load()` below, the functions will be executed and the result will be brought into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff33ab5-e8f4-47c8-9c17-2f222d00d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the function\n",
    "func=StatsNDVI(**config)\n",
    "\n",
    "# run the separate functions\n",
    "ndvi = func.input_data(datasets=dss, geobox=gbox)\n",
    "\n",
    "result = func.reduce(ndvi)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8edbb2-5d61-4544-b012-70557385f27d",
   "metadata": {},
   "source": [
    "### Bring into memory and plot\n",
    "\n",
    "This will take about 20 seconds to load with the default example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca5348-aba8-44a1-9a4a-022e6132734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.load()\n",
    "\n",
    "result['ndvi_median'].plot(vmin=0, vmax=0.7, size=5, add_labels=False)\n",
    "plt.title('Annual Median NDVI');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a27528-930b-4cc3-97bf-087c9e50e87e",
   "metadata": {},
   "source": [
    "## Section 2: Run the plugin using odc-stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b75dfd-e39b-4a53-8e8e-aa69004ee47d",
   "metadata": {},
   "source": [
    "### Saving tasks\n",
    "Before we can run a plugin with odc-stats, we need to extract datasets. In the sandbox (or a local machine), we would ordinarily do this by running [datacube.load](https://opendatacube.readthedocs.io/en/latest/api/indexed-data/generate/datacube.Datacube.load.html) (or sometimes `dea_tools.load_ard`). However, odc-stats works instead by caching a copy of the database to disk, thus providing a list of 'tasks' for odc-stats to run.  This is achieved with the function [odc-stats save-tasks]((https://github.com/opendatacube/odc-stats/blob/develop/odc/stats/_cli_save_tasks.py)). When run, this function will output three files:\n",
    "1. A .csv file listing all tasks for all the years in the database e.g., `ga_ls8c_ard_3_2017--P1Y.csv`\n",
    "2. A database cache file used by statistician when running jobs, e.g. `ga_ls8c_ard_3_2017--P1Y.db`\n",
    "3. A GeoJSON file per year, for visualising the prospective run e.g. `ga_ls8c_ard_3_2017--P1Y.geojson`\n",
    "\n",
    "The [save-tasks](https://github.com/opendatacube/odc-stats/blob/develop/odc/stats/_cli_save_tasks.py) function has a number of parameters that can be passed. Below we outline an example, and then list the main parameters.\n",
    "\n",
    "For example:\n",
    "\n",
    "    odc-stats save-tasks --frequency annual --grid au-extended-30 --year 2017 --input-products ga_ls8c_ard_3\n",
    "\n",
    "This would save tasks for all the Landsat 8 satellite imagery across Australia for the year 2017, on a 30m grid. `save-tasks` is quite flexible, so we can adjust these parameters to suit the kinds of product we're building: \n",
    "* **--input-products**: If, for example, we wanted to cache datasets from both Landsat 8 and Landsat 9, we can update the input-products parameter to read `ga_ls8c_ard_3-ga_ls9c_ard_3`, or in the case of sentinel-2, this could be `ga_s2am_ard_3-ga_s2bm_ard_3-ga_s2cm_ard_3`. In other cases, we may want to 'fuse' datasets where products require data from bands stored in multiple products. Products can be fused to use bands from both products in the derivative products, this creates a virtual product that contains the bands from both products. Note that for datasets to be fused they must have the same `center_time` and `region_code`. This process finds the matching dataset from each product that are in the same time and place and fuses them into one product.  An example of this is fc-percentiles, which uses the fractional cover bands in `ga_ls_fc_3` to calculate the percentiles, and uses the `ga_ls_wo_3` band to mask out bad data. The input-products parameter in this case looks like this: `ga_ls_fc_3+ga_ls_wo_3`.\n",
    "\n",
    "* **--frequency**: This determines the temporal binning of datasets. For example, for 3-month rolling we would use: `rolling-3months`. A list of supported values is [here](https://github.com/opendatacube/odc-stats/blob/7f34c86bdbd481340c41b5be7e0d0873ce3b3e1c/odc/stats/_cli_save_tasks.py#L24)\n",
    "* **--temporal-range**: Only extract datasets for a given time range. e.g. `2020-05--P1M` for the month of May in 2020, or `2017--P1Y`, will extract one years worth of data for 2017.\n",
    "* **--grid**: The spatial resolution and grid to use. For Australia this is `au_extended` plus a resolution, one of: `{10|20|30|60}`. e.g. for Sentinel-2 we would use `au_extended_10`.\n",
    "* **--gqa**: Only save datasets that pass `gqa_iterative_mean_xy <= gqa` test.\n",
    "* **--dataset-filter**: We can use this to filter based on metadata, for example: `{\"dataset_maturity\": \"final\"}`\n",
    "* **--year**: Use this flag as a shortcut for `--temporal-range=<int>--P1Y`, it will extract tasks for a single calendar year.\n",
    "\n",
    "***\n",
    "### Running save-tasks\n",
    "\n",
    "Let's run save-tasks in a way that mimics the datasets loading we did in Section 1.  The `!` will instruct the notebook to run this on the command line;  odc-stats is built to run through a command line interface.  You could also trigger this by wrapping the command in `os.system('odc-stats save-tasks ...')`.\n",
    "\n",
    "Note that this command will output datasets for all tiles in Australia. In the next step we will index the list of tiles so we only run one tile for testing.  Equally, we could have passed to save-tasks `--tiles <grid-index>` and it would only export datasets for a single tile or a list of tiles. We would need to know the index of the tile though for this to work.\n",
    "\n",
    "Remember, this will output three files: a `.db` file, a `.geosjson`, and a `.csv`\n",
    "\n",
    "This will take about a minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e408fe-8f46-4bb6-9f69-30a351083170",
   "metadata": {},
   "outputs": [],
   "source": [
    "!odc-stats save-tasks --frequency annual --grid au-extended-30 --temporal-range 2024--P1Y --input-products ga_ls8c_ard_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d60fa-f8df-49ad-b3c3-a5a7c7895f5a",
   "metadata": {},
   "source": [
    "### Find a tile to run\n",
    "\n",
    "Use the interactive map below to find a \"region_code\" to run (hover over a tile). Add the region_code numbers (e.g. `t = 36,17` if the region_code is 'x36y17') to the cell below the interactive map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc87a31-f778-47ae-b59b-197bcfc4193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf  = gpd.read_file('~/gdata1/projects/s2_gm/ga_ls8c_ard_3_2024--P1Y-2024--P1Y.geojson')\n",
    "\n",
    "gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9f5fd-4de3-4d82-bdb7-c78c8ac046d2",
   "metadata": {},
   "source": [
    "### Running ODC-Statistician\n",
    "\n",
    "First, set up a few parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e4888-31c7-4c54-a03e-d54eee27cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 36,17  # tile id to run i.e. x36y17\n",
    "resolution = 60 # can coarsen resolution to run to speed up testing\n",
    "results = '/gdata1/projects/s2_gm/results/' # where are we outputting resulting geotiffs? This could equally be an s3 path.\n",
    "name, version = 'ndvi_ls_median', '0-0-1' # product name and version (appended to results path)\n",
    "\n",
    "# Dask client parameters\n",
    "ncpus=7 #how many cpus to run on?\n",
    "mem='60Gi' # How much memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5169d15f-1204-4261-8d9d-6abc073471a5",
   "metadata": {},
   "source": [
    "#### Find the tile index to run\n",
    "\n",
    "We selected a region code, but the odc-stats \"run\" command expects a zero-based index to determine which tile to run.  Below we open the cached task database and find the index of the tile we want run.  We'll pass this index to odc-stats next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf6e78-f305-42ce-9f75-a314606e6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open the task database to find our tile, we need to create the OutputProduct class\n",
    "#  to open the taskdb but it doesn't do anything.\n",
    "op = OutputProduct(\n",
    "            name=name,\n",
    "            version=version,\n",
    "            short_name=name,\n",
    "            location=f\"s3://dummy-bucket/{name}/{version}\", #this is a fake path\n",
    "            properties={\"odc:file_format\": \"GeoTIFF\"},\n",
    "            measurements=['nbart_red'], #any measurements, doesn't matter.\n",
    "        )\n",
    "\n",
    "taskdb = TaskReader(f'ga_ls8c_ard_3_2024--P1Y.db', product=op)\n",
    "\n",
    "#select our individual task i.e. our tile\n",
    "task = taskdb.load_task((f'2024--P1Y', t[0], t[1]))\n",
    "\n",
    "# Now find index of the tile we want to run\n",
    "# We'll pass this index to odc-stats next\n",
    "tile_index_to_run = []\n",
    "all_tiles = list(taskdb.all_tiles)\n",
    "for i, index in zip(all_tiles, range(0, len(all_tiles))):\n",
    "    if (i[1]==t[0]) & (i[2]==t[1]):\n",
    "        tile_index_to_run.append(index)\n",
    "        print('Tile index =', index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b546fa85-deba-4fcd-9b65-4ec3a1ac1732",
   "metadata": {},
   "source": [
    "#### Optionally view tile to check location\n",
    "\n",
    "The next cell will plot the tile extent on an interactive map so you can ensure its the tile you want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84262d7-8e84-4859-abbf-e1935f9ab733",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(index=[0], crs='epsg:4326', geometry=[task.geobox.extent.to_crs('epsg:4326').geom])\n",
    "gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10a354-8256-4645-b9be-4c9ea92a750f",
   "metadata": {},
   "source": [
    "#### Running the plugin using odc-stats\n",
    "\n",
    "This is where it get's a little complicated. In order for `odc-stats` to 'see' our plugin, we need to put our plugin within an **installable python module** (note that [plugins](https://github.com/opendatacube/odc-stats/tree/develop/odc/stats/plugins) that are already within the odc-stats repository are available to use by default).  Similarly, we need to put our **configuration parameters into an external .yaml file**.  \n",
    "\n",
    "This has been done (its called `config_ndvi_ls_mean.yaml`), and below we open the config yaml to view its contents:\n",
    "\n",
    "The important parts are:\n",
    "* `plugin`: an import path to the installed python plugin\n",
    "* `plugin_config`: The parameters names and values that are passed to the plugin\n",
    "* `product`: These are key metadata fields for the product you're building\n",
    "\n",
    "The other parameters relate to batch runs on kubernetes, and the exported COG attributes such as compression levels etc.  These fields can usually be copied over from other similar product configs, such as this example for [Landsat geomedian](https://github.com/GeoscienceAustralia/dea-config/blob/3953ea18eee702a41867458c720e7480bd785c10/prod/services/odc-stats/geomedian/ga_ls8cls9c_gm_cyear_3.yaml). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c53f4-9d8a-4e25-9490-8b057f165b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path= 'config_ndvi_ls_median.yaml'\n",
    "\n",
    "with open(yaml_path, 'r') as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a34954-12db-4a78-9f96-5226a2d1265c",
   "metadata": {},
   "source": [
    "#### Install our python plugin\n",
    "\n",
    "Describing how to install a python package is beyond the scope of this notebook. A simple example of setting up an installable module on a local machine is [here](https://github.com/digitalearthafrica/crop-mask/tree/main/production/cm_tools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985fc078-c2c7-4608-aa0d-a2c0547e912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install s2_gm_tools/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e000559-1794-49d7-83fb-0373a4605770",
   "metadata": {},
   "source": [
    "#### Run odc-stats\n",
    "\n",
    "We will use `os.system('odc-stats run ...')` to call the command 'odc-stats run' so we can pass in variables defined earlier as python objects.\n",
    "\n",
    "\"odc-stats run\" has a number of parameters, some of which are described here. The key information to pass in is the name of the cached database file, the location of the config file, the output location, and the tile index to run.\n",
    "\n",
    "* **--filedb**: The name of the .db output by save-task e.g. \"ga_ls8c_ard_3_2024--P1Y.db\"\n",
    "* **--config**: Path to the config for plugin in yaml format\n",
    "* **--location**: Output location prefix as a uri or local file path: `s3://bucket/path/`\n",
    "* **--resolution**: Override output resolution, use this to speed up testing, e.g. '60'\n",
    "* **--threads**: Number of worker threads for the dask cluster, as an integer\n",
    "* **--memory-limit**: Limit memory used by Dask cluster, e.g. '100Gi'\n",
    "\n",
    "To view the progress of odc-stats, view the **dask dashboard**. Alter the email address to yours and use this link: \n",
    "\n",
    "https://app.sandbox.dea.ga.gov.au/user/chad.burton@ga.gov.au/proxy/8787/status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d988b07-8ae9-4bb6-8d93-6b3228a1587b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "os.system(\"odc-stats run \"\\\n",
    "          f\"--filedb=ga_ls8c_ard_3_2024--P1Y.db \"\\\n",
    "          f\"--config={yaml_path} \"\\\n",
    "          f\"--resolution={resolution} \"\\\n",
    "          f\"--threads={ncpus} \"\\\n",
    "          f\"--memory-limit={mem} \"\\\n",
    "          f\"--location=file:///home/jovyan/{results}{name}/{version} \" +str(tile_index_to_run[0])\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f4565-c92b-43ae-ac91-a4b9dc0f29da",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6046e-2332-4e8d-b1e2-6a1bdffbe64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= f'x{t[0]}'\n",
    "y= f'y{t[1]}'\n",
    "\n",
    "ndvi_path = f'{results}{name}/{version}/{x}/{y}/2024--P1Y/{name}_{x}{y}_2024--P1Y_final_ndvi_median.tif'\n",
    "ndvi=rxr.open_rasterio(ndvi_path).squeeze().drop_vars('band')\n",
    "ndvi=assign_crs(ndvi, crs='EPSG:3577')\n",
    "\n",
    "ndvi.plot(vmin=0, vmax=0.7, size=8, add_labels=False)\n",
    "plt.title('Annual Median NDVI');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f225904-1836-48af-8517-5bf3e5a7be41",
   "metadata": {},
   "source": [
    "## Remove all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f4585-8dc8-4981-be95-abf67a712f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r -f results/ndvi_ls_median/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d898abc-e658-4e68-bb4c-69f25a8e58b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
